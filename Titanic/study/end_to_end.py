# encoding = utf-8
# https://www.kaggle.com/massquantity/end-to-end-process-for-titanic-problem/notebook
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use("ggplot")

train = pd.read_csv("../data/train.csv")
test = pd.read_csv("../data/test.csv")
full = pd.concat([train, test], ignore_index=True)

# Embarked process
full.isnull().sum()
full.Embarked.mode()
full['Embarked'].fillna('S', inplace=True)

# Fare process
full.Fare.fillna(full[full.Pclass == 3]['Fare'].median(), inplace=True)

# Cabin process
full.loc[full.Cabin.notnull(), 'Cabin'] = 1
full.loc[full.Cabin.isnull(), 'Cabin'] = 0

pd.pivot_table(full, index=['Cabin'], values=['Survived']).plot.bar(figsize=(8, 5))
plt.title('Survival Rate')
plt.show()

cabin = pd.crosstab(full.Cabin, full.Survived)
cabin.rename(index={0: 'no cabin', 1: 'cabin'}, columns={0.0: 'Dead', 1.0: 'Survived'}, inplace=True)
cabin

cabin.plot.bar(figsize=(8, 5))
plt.xticks(rotation=0, size='xx-large')
plt.title('Survived Count')
plt.xlabel('')
plt.legend()
plt.show()

full['Title'] = full['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())
full.Title.value_counts()
pd.crosstab(full.Title, full.Sex)
full[(full.Title == 'Dr') & (full.Sex == 'female')]

nn = {'Capt': 'Rareman', 'Col': 'Rareman', 'Don': 'Rareman', 'Dona': 'Rarewoman',
      'Dr': 'Rareman', 'Jonkheer': 'Rareman', 'Lady': 'Rarewoman', 'Major': 'Rareman',
      'Master': 'Master', 'Miss': 'Miss', 'Mlle': 'Rarewoman', 'Mme': 'Rarewoman',
      'Mr': 'Mr', 'Mrs': 'Mrs', 'Ms': 'Rarewoman', 'Rev': 'Mr', 'Sir': 'Rareman',
      'the Countess': 'Rarewoman'}

full.Title = full.Title.map(nn)

# assign the female 'Dr' to 'Rarewoman'
full.loc[full.PassengerId == 797, 'Title'] = 'Rarewoman'
full.Title.value_counts()

full[full.Title == 'Master']['Sex'].value_counts()
full[full.Title == 'Master']['Age'].describe()

full[full.Title == 'Miss']['Age'].describe()
full.Age.fillna(999, inplace=True)


def girl(aa):
    if (aa.Age != 999) & (aa.Title == 'Miss') & (aa.Age <= 14):
        return 'Girl'
    elif (aa.Age == 999) & (aa.Title == 'Miss') & (aa.Parch != 0):
        return 'Girl'
    else:
        return aa.Title


full['Title'] = full.apply(girl, axis=1)
full.Title.value_counts()
full[full.Age == 999]['Age'].value_counts()
Tit = ['Mr', 'Miss', 'Mrs', 'Master', 'Girl', 'Rareman', 'Rarewoman']
for i in Tit:
    full.loc[(full.Age == 999) & (full.Title == i), 'Age'] = full.loc[full.Title == i, 'Age'].median()
full.info()
full.head()

full.groupby(['Title'])[['Age', 'Title']].mean().plot(kind='bar', figsize=(8, 5))
plt.xticks(rotation=0)
plt.show()

pd.crosstab(full.Sex, full.Survived).plot.bar(stacked=True, figsize=(8, 5), color=['#4169E1', '#FF00FF'])
plt.xticks(rotation=0, size='large')
plt.legend(bbox_to_anchor=(0.55, 0.9))

agehist = pd.concat([full[full.Survived == 1]['Age'], full[full.Survived == 0]['Age']], axis=1)
agehist.columns = ['Survived', 'Dead']
agehist.head()
agehist.plot(kind='hist', bins=30, figsize=(15, 8), alpha=0.3)

farehist = pd.concat([full[full.Survived == 1]['Fare'], full[full.Survived == 0]['Fare']], axis=1)
farehist.columns = ['Survived', 'Dead']
farehist.head()
farehist.plot.hist(bins=30, figsize=(15, 8), alpha=0.3, stacked=True, color=['blue', 'red'])

full.groupby(['Title'])[['Title', 'Survived']].mean().plot(kind='bar', figsize=(10, 7))
plt.xticks(rotation=0)

fig, axes = plt.subplots(2, 3, figsize=(15, 8))
Sex1 = ['male', 'female']
for i, ax in zip(Sex1, axes):
    for j, pp in zip(range(1, 4), ax):
        PclassSex = full[(full.Sex == i) & (full.Pclass == j)]['Survived'].value_counts().sort_index(ascending=False)
        pp.bar(range(len(PclassSex)), PclassSex, label=(i, 'Class' + str(j)))
        pp.set_xticks((0, 1))
        pp.set_xticklabels(('Survived', 'Dead'))
        pp.legend(bbox_to_anchor=(0.6, 1.1))

full.AgeCut = pd.cut(full.Age, 5)
full.FareCut = pd.qcut(full.Fare, 5)
full.AgeCut.value_counts().sort_index()
full.FareCut.value_counts().sort_index()

# replace agebands with ordinals
full.loc[full.Age <= 16.136, 'AgeCut'] = 1
full.loc[(full.Age > 16.136) & (full.Age <= 32.102), 'AgeCut'] = 2
full.loc[(full.Age > 32.102) & (full.Age <= 48.068), 'AgeCut'] = 3
full.loc[(full.Age > 48.068) & (full.Age <= 64.034), 'AgeCut'] = 4
full.loc[full.Age > 64.034, 'AgeCut'] = 5

full.loc[full.Fare <= 7.854, 'FareCut'] = 1
full.loc[(full.Fare > 7.854) & (full.Fare <= 10.5), 'FareCut'] = 2
full.loc[(full.Fare > 10.5) & (full.Fare <= 21.558), 'FareCut'] = 3
full.loc[(full.Fare > 21.558) & (full.Fare <= 41.579), 'FareCut'] = 4
full.loc[full.Fare > 41.579, 'FareCut'] = 5

full[['FareCut', 'Survived']].groupby(['FareCut']).mean().plot.bar(figsize=(8, 5))
plt.show()

full.corr()

full[full.Survived.notnull()].pivot_table(index=['Title', 'Pclass'], values=['Survived']).sort_values('Survived',
                                                                                                      ascending=False)
full[full.Survived.notnull()].pivot_table(index=['Title', 'Parch'], values=['Survived']).sort_values('Survived',
                                                                                                     ascending=False)
TPP = full[full.Survived.notnull()].pivot_table(index=['Title', 'Pclass', 'Parch'], values=['Survived']).sort_values(
    'Survived', ascending=False)
TPP

TPP.plot(kind='bar', figsize=(16, 10))
plt.xticks(rotation=40)
plt.axhline(0.8, color='#BA55D3')
plt.axhline(0.5, color='#BA55D3')
plt.annotate('80% survival rate', xy=(30, 0.81), xytext=(32, 0.85), arrowprops=dict(facecolor='#BA55D3', shrink=0.05))
plt.annotate('50% survival rate', xy=(32, 0.51), xytext=(34, 0.54), arrowprops=dict(facecolor='#BA55D3', shrink=0.05))
plt.show()

# use 'Title','Pclass','Parch' to generate feature 'TPP'.
Tit = ['Girl', 'Master', 'Mr', 'Miss', 'Mrs', 'Rareman', 'Rarewoman']
for i in Tit:
    for j in range(1, 4):
        for g in range(0, 10):
            if full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g) & (
            full.Survived.notnull()), 'Survived'].mean() >= 0.8:
                full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g), 'TPP'] = 1
            elif full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g) & (
            full.Survived.notnull()), 'Survived'].mean() >= 0.5:
                full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g), 'TPP'] = 2
            elif full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g) & (
            full.Survived.notnull()), 'Survived'].mean() >= 0:
                full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g), 'TPP'] = 3
            else:
                full.loc[(full.Title == i) & (full.Pclass == j) & (full.Parch == g), 'TPP'] = 4

full.ix[(full.TPP == 4) & (full.Sex == 'female') & (full.Pclass != 3), 'TPP'] = 1
full.ix[(full.TPP == 4) & (full.Sex == 'female') & (full.Pclass == 3), 'TPP'] = 2
full.ix[(full.TPP == 4) & (full.Sex == 'male') & (full.Pclass != 3), 'TPP'] = 2
full.ix[(full.TPP == 4) & (full.Sex == 'male') & (full.Pclass == 3), 'TPP'] = 3

full.TPP.value_counts()

predictors = ['Cabin', 'Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'AgeCut', 'TPP', 'FareCut', 'Age',
              'Fare']
full_dummies = pd.get_dummies(full[predictors])
full_dummies.head()

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC

models = [KNeighborsClassifier(), LogisticRegression(), GaussianNB(), DecisionTreeClassifier(),
          RandomForestClassifier(),
          GradientBoostingClassifier(), SVC()]

X = full_dummies[:891]
y = full.Survived[:891]
test_X = full_dummies[891:]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit(X).transform(X)
test_X_scaled = scaler.fit(X).transform(test_X)

# evaluate models by using cross-validation
names = ['KNN', 'LR', 'NB', 'Tree', 'RF', 'GDBT', 'SVM']
for name, model in zip(names, models):
    score = cross_val_score(model, X, y, cv=5)
    print("{}:{},{}".format(name, score.mean(), score))

# used scaled data
names = ['KNN', 'LR', 'NB', 'Tree', 'RF', 'GDBT', 'SVM']
for name, model in zip(names, models):
    score = cross_val_score(model, X_scaled, y, cv=5)
    print("{}:{},{}".format(name, score.mean(), score))

model = GradientBoostingClassifier()
model.fit(X, y)
model.feature_importances_

fi = pd.DataFrame({'importance': model.feature_importances_}, index=X.columns)
fi.sort_values('importance', ascending=False)

fi.sort_values('importance', ascending=False).plot.bar(figsize=(11, 7))
plt.xticks(rotation=30)
plt.title('Feature Importance', size='x-large')
plt.show()

# Now let's think through this problem in another way. Our goal here is to improve the overall accuracy. This is equivalent to minimizing the misclassified observations. So if all the misclassified observations are found, maybe we can see the pattern and generate some new features.
from sklearn.model_selection import KFold

kf = KFold(n_splits=10, random_state=1)
kf.get_n_splits(X)
print(kf)

# extract the indices of misclassified observations
rr = []
for train_index, val_index in kf.split(X):
    pred = model.fit(X.ix[train_index], y[train_index]).predict(X.ix[val_index])
    rr.append(y[val_index][pred != y[val_index]].index.values)

# combine all the indices
whole_index = np.concatenate(rr)
len(whole_index)
full.ix[whole_index].head()
diff = full.ix[whole_index]
diff.describe()
diff.describe(include=['O'])
# both mean and count of 'survived' should be considered.
diff.groupby(['Title'])['Survived'].agg([('average', 'mean'), ('number', 'count')])
diff.groupby(['Title', 'Pclass'])['Survived'].agg([('average', 'mean'), ('number', 'count')])
diff.groupby(['Title', 'Pclass', 'Parch', 'SibSp'])['Survived'].agg([('average', 'mean'), ('number', 'count')])

full.loc[
    (full.Title == 'Mr') & (full.Pclass == 1) & (full.Parch == 0) & ((full.SibSp == 0) | (full.SibSp == 1)), 'MPPS'] = 1
full.loc[(full.Title == 'Mr') & (full.Pclass != 1) & (full.Parch == 0) & (full.SibSp == 0), 'MPPS'] = 2
full.loc[(full.Title == 'Miss') & (full.Pclass == 3) & (full.Parch == 0) & (full.SibSp == 0), 'MPPS'] = 3
full.MPPS.fillna(4, inplace=True)
full.MPPS.value_counts()
diff[(diff.Title == 'Mr') | (diff.Title == 'Miss')].groupby(['Title', 'Survived', 'Pclass'])[
    ['Fare']].describe().unstack()
full[(full.Title == 'Mr') | (full.Title == 'Miss')].groupby(['Title', 'Survived', 'Pclass'])[
    ['Fare']].describe().unstack()
colormap = plt.cm.viridis
plt.figure(figsize=(12, 12))
plt.title('Pearson Correlation of Features', y=1.05, size=20)
sns.heatmap(
    full[['Cabin', 'Parch', 'Pclass', 'SibSp', 'AgeCut', 'TPP', 'FareCut', 'Age', 'Fare', 'MPPS', 'Survived']].astype(
        float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)
plt.show()

predictors = ['Cabin', 'Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'AgeCut', 'TPP', 'FareCut', 'Age',
              'Fare', 'MPPS']
full_dummies = pd.get_dummies(full[predictors])
X = full_dummies[:891]
y = full.Survived[:891]
test_X = full_dummies[891:]

scaler = StandardScaler()
X_scaled = scaler.fit(X).transform(X)
test_X_scaled = scaler.fit(X).transform(test_X)

from sklearn.model_selection import GridSearchCV

# k-Nearest Neighbors
param_grid = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# Logistic Regression
param_grid = {'C': [0.01, 0.1, 1, 10]}
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# second round grid search
param_grid = {'C': [0.04, 0.06, 0.08, 0.1, 0.12, 0.14]}
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# Support Vector Machine
param_grid = {'C': [0.01, 0.1, 1, 10], 'gamma': [0.01, 0.1, 1, 10]}
grid_search = GridSearchCV(SVC(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# second round grid search
param_grid = {'C': [2, 4, 6, 8, 10, 12, 14], 'gamma': [0.008, 0.01, 0.012, 0.015, 0.02]}
grid_search = GridSearchCV(SVC(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# Gradient Boosting Decision Tree
param_grid = {'n_estimators': [30, 50, 80, 120, 200], 'learning_rate': [0.05, 0.1, 0.5, 1],
              'max_depth': [1, 2, 3, 4, 5]}
grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# second round search
param_grid = {'n_estimators': [100, 120, 140, 160], 'learning_rate': [0.05, 0.08, 0.1, 0.12], 'max_depth': [3, 4]}
grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)

grid_search.fit(X_scaled, y)

grid_search.best_params_, grid_search.best_score_

# Ensemble Methods
# Bagging
from sklearn.ensemble import BaggingClassifier

bagging = BaggingClassifier(LogisticRegression(C=0.06), n_estimators=100)
from sklearn.ensemble import VotingClassifier

clf1 = LogisticRegression(C=0.06)
clf2 = RandomForestClassifier(n_estimators=500)
clf3 = GradientBoostingClassifier(n_estimators=100, learning_rate=0.12, max_depth=4)
clf4 = SVC(C=4, gamma=0.015, probability=True)
clf5 = KNeighborsClassifier(n_neighbors=8)
eclf_hard = VotingClassifier(estimators=[('LR', clf1), ('RF', clf2), ('GDBT', clf3), ('SVM', clf4), ('KNN', clf5)])
# add weights
eclfW_hard = VotingClassifier(estimators=[('LR', clf1), ('RF', clf2), ('GDBT', clf3), ('SVM', clf4), ('KNN', clf5)],
                              weights=[1, 1, 2, 2, 1])
# soft voting
eclf_soft = VotingClassifier(estimators=[('LR', clf1), ('RF', clf2), ('GDBT', clf3), ('SVM', clf4), ('KNN', clf5)],
                             voting='soft')
# add weights
eclfW_soft = VotingClassifier(estimators=[('LR', clf1), ('RF', clf2), ('GDBT', clf3), ('SVM', clf4), ('KNN', clf5)],
                              voting='soft', weights=[1, 1, 2, 2, 1])

models = [KNeighborsClassifier(n_neighbors=8), LogisticRegression(C=0.06), GaussianNB(), DecisionTreeClassifier(),
          RandomForestClassifier(n_estimators=500),
          GradientBoostingClassifier(n_estimators=100, learning_rate=0.12, max_depth=4), SVC(C=4, gamma=0.015),
          eclf_hard, eclf_soft, eclfW_hard, eclfW_soft, bagging]

names = ['KNN', 'LR', 'NB', 'CART', 'RF', 'GBT', 'SVM', 'VC_hard', 'VC_soft', 'VCW_hard', 'VCW_soft', 'Bagging']
for name, model in zip(names, models):
    score = cross_val_score(model, X_scaled, y, cv=5)
    print("{}: {},{}".format(name, score.mean(), score))

# Stacking
from sklearn.model_selection import StratifiedKFold

n_train = train.shape[0]
n_test = test.shape[0]
kf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)


def get_oof(clf, X, y, test_X):
    oof_train = np.zeros((n_train,))
    oof_test_mean = np.zeros((n_test,))
    oof_test_single = np.empty((5, n_test))
    for i, (train_index, val_index) in enumerate(kf.split(X, y)):
        kf_X_train = X[train_index]
        kf_y_train = y[train_index]
        kf_X_val = X[val_index]
        clf.fit(kf_X_train, kf_y_train)
        oof_train[val_index] = clf.predict(kf_X_val)
        oof_test_single[i, :] = clf.predict(test_X)
    oof_test_mean = oof_test_single.mean(axis=0)
    return oof_train.reshape(-1, 1), oof_test_mean.reshape(-1, 1)


LR_train, LR_test = get_oof(LogisticRegression(C=0.06), X_scaled, y, test_X_scaled)
KNN_train, KNN_test = get_oof(KNeighborsClassifier(n_neighbors=8), X_scaled, y, test_X_scaled)
SVM_train, SVM_test = get_oof(SVC(C=4, gamma=0.015), X_scaled, y, test_X_scaled)
GBDT_train, GBDT_test = get_oof(GradientBoostingClassifier(n_estimators=100, learning_rate=0.12, max_depth=4), X_scaled,
                                y, test_X_scaled)
X_stack = np.concatenate((LR_train, KNN_train, SVM_train, GBDT_train), axis=1)
y_stack = y
X_test_stack = np.concatenate((LR_test, KNN_test, SVM_test, GBDT_test), axis=1)
X_stack.shape, y_stack.shape, X_test_stack.shape
stack_score = cross_val_score(RandomForestClassifier(n_estimators=1000), X_stack, y_stack, cv=5)
# cross-validation score of stacking
stack_score.mean(), stack_score

pred = RandomForestClassifier(n_estimators=500).fit(X_stack, y_stack).predict(X_test_stack)
tt = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': [int(x) for x in pred]})
tt.to_csv('G.csv', index=False)
